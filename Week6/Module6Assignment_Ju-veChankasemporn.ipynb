{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d4aa350",
      "metadata": {
        "id": "3d4aa350"
      },
      "source": [
        "# Generate Synthetic Training Data\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/06/notebooks/generate-synthetic.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/06/notebooks/generate-synthetic.ipynb\">\n",
        "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5572879c",
      "metadata": {
        "id": "5572879c"
      },
      "source": [
        "## Data generation settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "d29c9393",
      "metadata": {
        "id": "d29c9393"
      },
      "outputs": [],
      "source": [
        "NUM_TRAIN_EXAMPLES = 1000  # @param {type:\"number\"}\n",
        "NUM_VAL_EXAMPLES = 200  # @param {type:\"number\"}\n",
        "NUM_TEST_EXAMPLES = 20 # @param {type:\"number\"}\n",
        "TEMPERATURE = 0.8  # @param {type:\"number\"}\n",
        "\n",
        "DATA_FOLDER = \"./.data/generated\"\n",
        "!mkdir -p {DATA_FOLDER}\n",
        "\n",
        "DATAGEN_MODEL = \"openai/gpt-5-nano\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672c9b3d",
      "metadata": {
        "id": "672c9b3d"
      },
      "source": [
        "## Dataset diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "a485a8de",
      "metadata": {
        "id": "a485a8de"
      },
      "outputs": [],
      "source": [
        "TOPICS = [\n",
        "  \"Anti-air consistency (DP vs normal vs air-to-air)\",\n",
        "  \"Whiff punishing basics\",\n",
        "  \"Hit-confirming lights into special\",\n",
        "  \"Strike/throw offense\",\n",
        "  \"Meaty timing after knockdown\",\n",
        "  \"Defense: blocking high/low + delay tech\",\n",
        "  \"Throw tech vs shimmy awareness\",\n",
        "  \"Drive Impact reactions and counters\",\n",
        "  \"Drive Rush pressure and checking DR\",\n",
        "  \"Corner control and corner escape\",\n",
        "  \"Meter management (Drive Gauge + Super)\",\n",
        "  \"Matchup basics vs shotos (Ryu/Ken/Luke/Sagat)\",\n",
        "  \"Matchup basics vs grapplers (Gief/Manon/Lily)\",\n",
        "  \"Matchup basics vs rush-down/mix-ups (Kimberly/Jamie/Elena)\",\n",
        "  \"Matchup basics vs zoners (Dhalsim/JP)\",\n",
        "  \"Matchup basics vs charged characters (Honda/Blanka/Bison/Guile/Deejay)\",\n",
        "  \"Neutral: spacing with pokes\",\n",
        "  \"Punish combos: light/medium/heavy starters\",\n",
        "  \"Mental stack + decision making\",\n",
        "]\n",
        "\n",
        "PLAYER_RANK = [\n",
        "    [\"Rookie\", \"Iron\", \"Bronze\", \"Silver\", \"Gold\"],\n",
        "    [\"Platinum\", \"Diamond\"],\n",
        "    [\"1300MR\", \"1400MR\"],\n",
        "    [\"1500MR\", \"1600MR\", \"1700MR\", \"1800MR\"],\n",
        "]\n",
        "\n",
        "PLAYER_RANK_WEIGHTS = [0.25, 0.25, 0.25, 0.25]\n",
        "\n",
        "CHARACTERS = [\n",
        "    \"Ryu\", \"Ken\", \"Luke\", \"Juri\", \"Chun-Li\", \"Cammy\",\n",
        "    \"Zangief\", \"Manon\", \"Marisa\", \"JP\", \"Guile\",\n",
        "    \"Ed\", \"Dhalsim\", \"Lily\", \"Rashid\", \"Terry\", \"Mai\",\n",
        "    \"Sagat\", \"Elena\", \"Honda\", \"Kimberly\",\n",
        "    \"AKI\", \"Bison\", \"Akuma\", \"Deejay\", \"Jamie\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "421c49e4",
      "metadata": {
        "id": "421c49e4"
      },
      "source": [
        "## Model for structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "91787f37",
      "metadata": {
        "id": "91787f37"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import List, Literal, Optional\n",
        "\n",
        "class SF6CoachAnswer(BaseModel):\n",
        "    student_goal: str\n",
        "    situation_summary: str\n",
        "    diagnosis: List[str]              # what's going wrong\n",
        "    key_concepts: List[str]           # what to learn\n",
        "    gameplan: List[str]               # actionable plan\n",
        "    drills: List[str]                 # training mode drills\n",
        "    mistakes_to_avoid: List[str]\n",
        "    quick_cheatsheet: List[str]       # 1-liners\n",
        "    confidence_note: str              # supportive, short\n",
        "\n",
        "class SF6Conversation(BaseModel):\n",
        "    user: str\n",
        "    assistant: SF6CoachAnswer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712a3900",
      "metadata": {
        "id": "712a3900"
      },
      "source": [
        "## Get OpenRouter API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "bc6fd261",
      "metadata": {
        "id": "bc6fd261"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import userdata # type:ignore\n",
        "  os.environ['OPENROUTER_API_KEY'] = userdata.get('OpenRouter')\n",
        "else:\n",
        "  load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218aa9db",
      "metadata": {
        "id": "218aa9db"
      },
      "source": [
        "## Conversation generation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "1ce9bb29",
      "metadata": {
        "id": "1ce9bb29"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "def generate_completion(prompt: str) -> SF6Conversation | None:\n",
        "    response = client.responses.parse(\n",
        "        model=DATAGEN_MODEL,\n",
        "        input=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=TEMPERATURE,\n",
        "        stream=False,\n",
        "        text_format=SF6Conversation\n",
        "    )\n",
        "\n",
        "    return response.output_parsed\n",
        "\n",
        "def create_conversation(\n",
        "    topic: str,\n",
        "    player_rank: str,\n",
        "    player_character: str,\n",
        "    opponent_character: str\n",
        ") -> SF6Conversation | None:\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a Street Fighter 6 coach.\n",
        "\n",
        "    Topic: {topic}\n",
        "    Student rank: {player_rank}\n",
        "    Student character: {player_character}\n",
        "    Opponent character: {opponent_character}\n",
        "\n",
        "    Important rules:\n",
        "    - Advice MUST reflect matchup specifics.\n",
        "    - Advice level MUST match the student rank.\n",
        "      - Lower ranks → fundamentals, simple concepts.\n",
        "      - Master MR → advanced spacing traps, conditioning, frame traps, option coverage, mental stack.\n",
        "\n",
        "    Return ONLY valid JSON matching this schema exactly:\n",
        "\n",
        "    {{\n",
        "      \"user\": \"1-4 sentence realistic student message written from the player's perspective\",\n",
        "      \"assistant\": {{\n",
        "        \"diagnosis\": [\"3-6 bullet strings\"],\n",
        "        \"gameplan\": [\"3-6 bullet strings\"],\n",
        "        \"drills\": [\"2-5 bullet strings\"],\n",
        "        \"mistakes_to_avoid\": [\"exactly 3 bullet strings\"],\n",
        "        \"encouraging_line\": \"one short sentence\"\n",
        "      }}\n",
        "    }}\n",
        "\n",
        "    Do not wrap in code fences.\n",
        "    Do not add extra keys.\n",
        "    \"\"\"\n",
        "\n",
        "    return generate_completion(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97fbaaf3",
      "metadata": {
        "id": "97fbaaf3"
      },
      "source": [
        "## Dataset generation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "e79b6bfc",
      "metadata": {
        "id": "e79b6bfc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_dataset_files(num_examples: int, jsonl_path: str, pretty_json_path: str) -> None:\n",
        "    examples = []\n",
        "\n",
        "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f_jsonl:\n",
        "        for idx in tqdm(range(num_examples)):\n",
        "\n",
        "            topic = random.choice(TOPICS)\n",
        "\n",
        "            # Rank selection\n",
        "            rank_bucket = random.choices(\n",
        "                PLAYER_RANK,\n",
        "                weights=PLAYER_RANK_WEIGHTS\n",
        "            )[0]\n",
        "            player_rank = random.choice(rank_bucket)\n",
        "\n",
        "            # Player character\n",
        "            player_character = random.choice(CHARACTERS)\n",
        "\n",
        "            # Opponent character (avoid mirror unless desired)\n",
        "            opponent_character = random.choice(\n",
        "                [c for c in CHARACTERS if c != player_character]\n",
        "            )\n",
        "\n",
        "            conversation = None\n",
        "            tries = 0\n",
        "\n",
        "            while conversation is None and tries < 8:\n",
        "                conversation = create_conversation(\n",
        "                    topic,\n",
        "                    player_rank,\n",
        "                    player_character,\n",
        "                    opponent_character\n",
        "                )\n",
        "                tries += 1\n",
        "\n",
        "            if conversation is None:\n",
        "                print(f\"Failed generating example {idx}\")\n",
        "                continue\n",
        "\n",
        "            record = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": conversation.user},\n",
        "                    {\"role\": \"assistant\", \"content\": conversation.assistant.model_dump()},\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            f_jsonl.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "            examples.append(record)\n",
        "\n",
        "    with open(pretty_json_path, \"w\", encoding=\"utf-8\") as f_pretty:\n",
        "        json.dump(examples, f_pretty, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98fa54d",
      "metadata": {
        "id": "d98fa54d"
      },
      "source": [
        "## Generate all the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "4fc06c35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "4fc06c35",
        "outputId": "a43ee30b-960f-45fe-8621-3db62d1c10b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "APIStatusError",
          "evalue": "Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 65536 tokens, but can only afford 63580. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_38MRPonC4sZPj8bVTjO62Z2wqmb'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1484871344.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_pretty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{DATA_FOLDER}/test_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgenerate_dataset_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAIN_EXAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pretty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mgenerate_dataset_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_VAL_EXAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALID_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pretty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgenerate_dataset_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TEST_EXAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pretty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3173969638.py\u001b[0m in \u001b[0;36mgenerate_dataset_files\u001b[0;34m(num_examples, jsonl_path, pretty_json_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mconversation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtries\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 conversation = create_conversation(\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mplayer_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3695836369.py\u001b[0m in \u001b[0;36mcreate_conversation\u001b[0;34m(topic, player_rank, player_character, opponent_character)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3695836369.py\u001b[0m in \u001b[0;36mgenerate_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSF6Conversation\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     response = client.responses.parse(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATAGEN_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/responses/responses.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text_format, background, context_management, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, verbosity, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1188\u001b[0m             )\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0;34m\"/responses\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[0;32m-> 1297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 65536 tokens, but can only afford 63580. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_38MRPonC4sZPj8bVTjO62Z2wqmb'}"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TRAIN_FILE = f\"{DATA_FOLDER}/train_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.jsonl\"\n",
        "train_pretty = f\"{DATA_FOLDER}/train_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.pretty.json\"\n",
        "VALID_FILE = f\"{DATA_FOLDER}/valid_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.jsonl\"\n",
        "valid_pretty = f\"{DATA_FOLDER}/train_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.pretty.json\"\n",
        "TEST_FILE = f\"{DATA_FOLDER}/test_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.jsonl\"\n",
        "test_pretty = f\"{DATA_FOLDER}/test_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.jsonl\"\n",
        "\n",
        "generate_dataset_files(NUM_TRAIN_EXAMPLES, TRAIN_FILE, train_pretty)\n",
        "generate_dataset_files(NUM_VAL_EXAMPLES, VALID_FILE, valid_pretty)\n",
        "generate_dataset_files(NUM_TEST_EXAMPLES, test_pretty)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}